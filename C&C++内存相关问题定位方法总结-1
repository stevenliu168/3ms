C&C++内存相关问题定位方法总结
日期：2017-10-26 08:54浏览：4130评论：7
前言
前几年一直在从事C++的开发，中间遇到了各种内存的问题，比如内存泄露，内存越界，内存空洞等等，把中间的方法和思想都总结一下，希望可以给大家定位C/C++的内存问题提供一些思路。

本文先介绍了glibc的ptmalloc原理，然后介绍了常用的内存定位API，命令和工具，最后具体介绍一下如何定位内存泄露，越界和空洞等问题。 如果大家有什么好的方法，也欢迎一起交流。

linux 32和64位进程虚拟内存示意图
在Linux 2.6.7以后32位机器上进程的内存模型如下：
image.png
对于32位机器，我们的进程被loader装载到内存时，其虚拟内存布局如上图所示。首先被载入的是.text段（放程序代码的），然后是.data段（存放在编译阶段就能确定的数据，也就是通常所说的静态存储区，赋了初值的全局变量、静态变量以及常量放在这个区域），最后是.bss段（定义而没有赋初值的全局变量和静态变量放在这个区域）。程序能访问的最后地址是0xbfffffff，也就是3G地址处，3G以上的1G空间是内核使用的，应用程序不可以直接访问。应用程序的stack（存放局部变量等，linux下，堆栈大小可以通过unlimit –a查询）从最高地址开始向下扩展。在.bss段和stack之间分为了两个部分：MAP和HEAP，程序中通过malloc和free对内存的操作就是在这一部分。

64位进程虚拟内存布局：
image.png

glibc的ptmalloc原理(对于本章节不感兴趣的可以跳过)
在介绍内存问题之前，要先了解一下ptmalloc的原理，了解底层是如何管理内存的。
三层管理机制：
image.png

Main_arena与non_main_arena
在内存分配器中只有一个主分配区，每次分配内存都会对主分配去加锁。为了支持多线程，增加非主分配区的支持。
主分配区可以访问进程的heap区域和mmap映射区域，而非主分配区只能访问mmap映射区域。当某一线程需要调用malloc()分配内存空间时，该线程先查看线程私有变量中是否已经存在一个分配区，如果存在，尝试对该分配区加锁，如果加锁成功，使用该分配区分配内存，如果失败，该线程搜索循环链表试图获得一个没有加锁的分配区。如果所有的分配区都已经加锁，那么malloc()会开辟一个新的分配区，把该分配区加入到全局分配区循环链表并加锁，然后使用该分配区进行分配内存操作。在释放操作中，线程同样试图获得待释放内存块所在分配区的锁，如果该分配区正在被别的线程使用，则需要等待直到其他线程释放该分配区的互斥锁之后才可以进行释放操作。

Glibc中分配区的实现
每个分配区是 struct malloc_state 的一个实例， ptmalloc 使用 malloc_state 来管理分配。
stuct  malloc_state 的定义如下：
struct malloc_state {
  /* Serialize access.  */
  mutex_t mutex;

  /* Flags (formerly in max_fast).  */
  int flags;

#if THREAD_STATS
  /* Statistics for locking.  Only used if THREAD_STATS is defined.  */
  long stat_lock_direct, stat_lock_loop, stat_lock_wait;
#endif

  /* Fastbins */
  mfastbinptr      fastbinsY[NFASTBINS];

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr        top;

  /* The remainder from the most recent split of a small request */
  mchunkptr        last_remainder;

  /* Normal bins packed as described above */
  mchunkptr        bins[NBINS * 2 - 2];

  /* Bitmap of bins */
  unsigned int     binmap[BINMAPSIZE];

  /* Linked list */
  struct malloc_state *next;
#ifdef PER_THREAD
  /* Linked list for free arenas.  */
  struct malloc_state *next_free;
#endif


  /* Memory allocated from the system in this arena.  */
  INTERNAL_SIZE_T system_mem;
  INTERNAL_SIZE_T max_system_mem;
};
Mutex 用于串行化访问分配区，当有多个线程访问同一个分配区时，第一个获得这个 mutex 的线程将使用该分配区分配内存，分配完成后，释放该分配区的 mutex ，以便其它线程使用该分配区。
Flags 记录了分配区的一些标志， bit0 用于标识分配区是否包含至少一个 fast bin chunk ， bit1 用于标识分配区是否能返回连续的虚拟地址空间。
fastbinsY 拥有 10 （ NFASTBINS ）个元素的数组，用于存放每个 fast chunk 链表头指针，所以 fast bins 最多包含 10 个 fast chunk 的单向链表。
top 是一个 chunk 指针，指向分配区的 top chunk 。
last_remainder 是一个 chunk 指针，分配区上次分配 small chunk 时，从一个 chunk 中分裂出一个 small chunk返回给用户，分裂后的剩余部分形成一个 chunk ， last_remainder 就是指向的这个 chunk 。
bins 用于存储 unstored bin ， small bins 和 large bins 的 chunk 链表头， small bins 一共 62 个， large bins 一共 63 个，加起来一共 125 个 bin 。而 NBINS 定义为 128 ，其实 bin[0] 和 bin[127] 都不存在， bin[1] 为 unsorted bin 的 chunk 链表头，所以实际只有 126bins 。 Bins 数组能存放了 254 （ NBINS * 2 – 2 ）个 mchunkptr 指针，而我们实现需要存储 chunk 的实例，一般情况下， chunk 实例的大小为 6 个 mchunkptr 大小，这 254 个指针的大小怎么能存下 126 个 chunk 呢？这里使用了一个技巧，如果按照我们的常规想法，也许会申请 126 个 malloc_chunk 结构体指针元素的数组，然后再给链表申请一个头节点（即 126 个），再让每个指针元素正确指向而形成 126 个具有头节点的链表。事实上，对于 malloc_chunk 类型的链表“头节点”，其内的 prev_size 和 size 字段是没有任何实际作用的，fd_nextsize 和 bk_nextsize 字段只有 large bins 中的空闲 chunk 才会用到，而对于 large bins 的空闲 chunk 链表头不需要这两个字段，因此这四个字段所占空间如果不合理使用的话那就是白白的浪费。我们再来看一看 128 个malloc_chunk 结构体指针元素的数组占了多少内存空间呢？假设 SIZE_SZ 的大小为 8B ，则指针的大小也为 8B ，结果为 12628=2016 字节。而 126 个 malloc_chunk 类型的链表“头节点”需要多少内存呢？ 12668=6048 ，真的是 6048B 么？不是，刚才不是说了， prev_size ， size ， fd_nextsize 和 bk_nextsize 这四个字段是没有任何实际作用的，因此完全可以被重用（覆盖），因此实际需要内存为 12628=2016 。 Bins 指针数组的大小为，（ 1282-2） 8=2032,2032 大于 2016 （事实上最后 16 个字节都被浪费掉了），那么这 254 个 malloc_chunk 结构体指针元素数组所占内存空间就可以存储这 126 个头节点了。
binmap 字段是一个 int 数组， ptmalloc 用一个 bit 来标识该 bit 对应的 bin 中是否包含空闲 chunk 。 binmap一共 128bit ， 16 字节， 4 个 int 大小， binmap 按 int 分成 4 个 block ，每个 block 有 32 个 bit ，根据 bin indx 可以使用宏 idx2block 计算出该 bin 在 binmap 对应的 bit 属于哪个 block 。 idx2bit 宏取第 i 位为 1 ，其它位都为 0 的掩码，举个例子： idx2bit(3) 为 “ 0000 1000 ”（只显示 8 位）。 mark_bin 设置第 i 个 bin 在 binmap中对应的 bit 位为 1 ； unmark_bin 设置第 i 个 bin 在 binmap 中对应的 bit 位为 0 ； get_binmap 获取第 i 个 bin 在binmap 中对应的 bit 。
next 字段用于将分配区以单向链表链接起来。
next_free 字段空闲的分配区链接在单向链表中，只有在定义了 PER_THREAD 的情况下才定义该字段。
system_mem 字段记录了当前分配区已经分配的内存大小。
max_system_mem 记录了当前分配区最大能分配的内存大小。
Chunck的简介
image.png

特殊的chunck
Top Chunk
为内存是按地址从低向高进行分配的，在空闲内存的最高处，必然存在着一块空闲chunk，叫做top chunk。

主分配区是唯一能够映射进程heap区域的分配区，它可以通过sbrk()来增大或是收缩进程heap的大小，ptmalloc在开始时会预先分配一块较大的空闲内存（也就是所谓的 heap），主分配区的top chunk在第一次调用malloc时会分配一块(chunk_size + 128KB) align 4KB大小的空间作为初始的heap，用户从top chunk分配内存时，可以直接取出一块内存给用户。在回收内存时，回收的内存恰好与top chunk相邻则合并成新的top chunk，当该次回收的空闲内存大小达到某个阈值，并且top chunk的大小也超过了收缩阈值，会执行内存收缩，减小top chunk的大小，但至少要保留一个页大小的空闲内存，从而把内存归还给操作系统。如果向主分配区的top chunk申请内存，而top chunk中没有空闲内存，ptmalloc会调用sbrk()将的进程heap的边界brk上移，然后修改top chunk的大小。

非主分配区会预先从mmap区域分配一块较大的空闲内存模拟sub-heap，通过管理sub-heap来响应用户的需求。当bins和fast bins都不能满足分配需要的时候，ptmalloc会设法在top chunk中分出一块内存给用户，如果top chunk本身不够大，分配程序会重新分配一个sub-heap，并将top chunk迁移到新的sub-heap上，新的sub-heap与已有的sub-heap用单向链表连接起来，然后在新的top chunk上分配所需的内存以满足分配的需要，实际上，top chunk在分配时总是在fast bins和bins之后被考虑，所以，不论top chunk有多大，它都不会被放到fast bins或者是bins中。Top chunk的大小是随着分配和回收不停变换的，如果从top chunk分配内存会导致top chunk减小，如果回收的chunk恰好与top chunk相邻，那么这两个chunk就会合并成新的top chunk，从而使top chunk变大。如果在free时回收的内存大于某个阈值，并且top chunk的大小也超过了收缩阈值，ptmalloc会收缩sub-heap，如果top-chunk包含了整个sub-heap，ptmalloc会调用munmap把整个sub-heap的内存返回给操作系统。

mmaped chunk
当需要分配的chunk足够大，而且fast bins和bins都不能满足要求，甚至top chunk本身也不能满足分配需求时，ptmalloc会使用mmap来直接使用内存映射来将页映射到进程空间。这样分配的chunk在被free时将直接解除映射，于是就将内存归还给了操作系统，再次对这样的内存区的引用将导致segmentation fault错误。这样的chunk也不会包含在任何bin中。

last remainder
Last remainder是另外一种特殊的chunk，就像top chunk和mmaped chunk一样，不会在任何bins中找到这种chunk。当需要分配一个small chunk，但在small bins中找不到合适的chunk，如果last remainder chunk的大小大于所需的small chunk大小，last remainder chunk被分裂成两个chunk，其中一个chunk返回给用户，另一个chunk变成新的last remainder chuk。

bins的简介
相似大小的chunk用双向链表链接起来，这样的一个链表被称为一个bin，glibc一共有128个bin，并使用一个数组来存储这些bin（如下图所示）。
image.png

Fast bins: 不大于max_fast （默认值为64B）的chunk被释放后，首先会被放到fast bins 中，
Unsorted Bin : unsorted bin的队列使用bins数组的第一个，如果被用户释放的chunk大于max_fast，或者fast bins中的空闲chunk合并后，这些chunk首先会被放到unsorted bin队列中，在进行malloc操作的时候，如果在fast bins中没有找到合适的chunk，则ptmalloc会先在unsorted bin中查找合适的空闲chunk，然后才查找bins。如果unsorted bin不能满足分配要求。malloc便会将unsorted bin中的chunk加入bins中。然后再从bins中继续进行查找和分配过程
glibc内存分配过程
小于等于64字节：用pool算法分配。
64到512字节之间：在最佳匹配算法分配和pool算法分配中取一种合适的。
大于等于512字节：用最佳匹配算法分配。
大于等于mmap分配阈值（默认值128KB）：根据设置的mmap的分配策略进行分配，如果没有开启mmap分配阈值的动态调整机制，大于等于128KB就直接调用mmap分配。否则，大于等于mmap分配阈值时才直接调用mmap()分配。
分配过程：
image.png
1) 获取分配区的锁，为了防止多个线程同时访问同一个分配区，在进行分配之前需要取得分配区域的锁。线程先查看线程私有实例中是否已经存在一个分配区，如果存在尝试对该分配区加锁，如果加锁成功，使用该分配区分配内存，否则，该线程搜索分配区循环链表试图获得一个空闲（没有加锁）的分配区。如果所有的分配区都已经加锁，那么ptmalloc会开辟一个新的分配区，把该分配区加入到全局分配区循环链表和线程的私有实例中并加锁，然后使用该分配区进行分配操作。开辟出来的新分配区一定为非主分配区，因为主分配区是从父进程那里继承来的。开辟非主分配区时会调用mmap()创建一个sub-heap，并设置好top chunk。

2) 将用户的请求大小转换为实际需要分配的chunk空间大小。

3) 判断所需分配chunk的大小是否满足chunk_size <= max_fast (max_fast 默认为 64B)，如果是的话，则转下一步，否则跳到第5步。

4) 首先尝试在fast bins中取一个所需大小的chunk分配给用户。如果可以找到，则分配结束。否则转到下一步。

5) 判断所需大小是否处在small bins中，即判断chunk_size < 512B是否成立。如果chunk大小处在small bins中，则转下一步，否则转到第6步。

6) 根据所需分配的chunk的大小，找到具体所在的某个small bin，从该bin的尾部摘取一个恰好满足大小的chunk。若成功，则分配结束，否则，转到下一步。

7) 到了这一步，说明需要分配的是一块大的内存，或者small bins中找不到合适的 chunk。于是，ptmalloc首先会遍历fast bins中的chunk，将相邻的chunk进行合并，并链接到unsorted bin中，然后遍历unsorted bin中的chunk，如果unsorted bin只有一个chunk，并且这个chunk在上次分配时被使用过，并且所需分配的chunk大小属于small bins，并且chunk的大小大于等于需要分配的大小，这种情况下就直接将该chunk进行切割，分配结束，否则将根据chunk的空间大小将其放入small bins或是large bins中，遍历完成后，转入下一步。

8) 到了这一步，说明需要分配的是一块大的内存，或者small bins和unsorted bin中都找不到合适的 chunk，并且fast bins和unsorted bin中所有的chunk都清除干净了。从large bins中按照“smallest-first，best-fit”原则，找一个合适的 chunk，从中划分一块所需大小的chunk，并将剩下的部分链接回到bins中。若操作成功，则分配结束，否则转到下一步。

9) 如果搜索fast bins和bins都没有找到合适的chunk，那么就需要操作top chunk来进行分配了。判断top chunk大小是否满足所需chunk的大小，如果是，则从top chunk中分出一块来。否则转到下一步。

10) 到了这一步，说明top chunk也不能满足分配要求，所以，于是就有了两个选择: 如果是主分配区，调用sbrk()，增加top chunk大小；如果是非主分配区，调用mmap来分配一个新的sub-heap，增加top chunk大小；或者使用mmap()来直接分配。在这里，需要依靠chunk的大小来决定到底使用哪种方法。判断所需分配的chunk大小是否大于等于 mmap分配阈值，如果是的话，则转下一步，调用mmap分配，否则跳到第12步，增加top chunk 的大小。

11) 使用mmap系统调用为程序的内存空间映射一块chunk_size align 4kB大小的空间。 然后将内存指针返回给用户。

12) 判断是否为第一次调用malloc，若是主分配区，则需要进行一次初始化工作，分配

13) 一块大小为(chunk_size + 128KB) align 4KB大小的空间作为初始的heap。若已经初始化过了，主分配区则调用sbrk()增加heap空间，分主分配区则在top chunk中切割出一个chunk，使之满足分配需求，并将内存指针返回给用户。

glibc内存释放过程
image.png
1) free()函数同样首先需要获取分配区的锁，来保证线程安全。

2) 判断传入的指针是否为0，如果为0，则什么都不做，直接return。否则转下一步。

3) 判断所需释放的chunk是否为mmaped chunk，如果是，则调用munmap()释放mmaped chunk，解除内存空间映射，该该空间不再有效。如果开启了mmap分配阈值的动态调整机制，并且当前回收的chunk大小大于mmap分配阈值，将mmap分配阈值设置为该chunk的大小，将mmap收缩阈值设定为mmap分配阈值的2倍，释放完成，否则跳到下一步。

4) 判断chunk的大小和所处的位置，若chunk_size <= max_fast，并且chunk并不位于heap的顶部，也就是说并不与top chunk相邻，则转到下一步，否则跳到第6步。（因为与top chunk相邻的小chunk也和 top chunk进行合并，所以这里不仅需要判断大小，还需要判断相邻情况）

5) 将chunk放到fast bins中，chunk放入到fast bins中时，并不修改该chunk使用状态位P。也不与相邻的chunk进行合并。只是放进去，如此而已。这一步做完之后释放便结束了，程序从free()函数中返回。

6) 判断前一个chunk是否处在使用中，如果前一个块也是空闲块，则合并。并转下一步。

7) 判断当前释放chunk的下一个块是否为top chunk，如果是，则转第9步，否则转下一步。

8) 判断下一个chunk是否处在使用中，如果下一个chunk也是空闲的，则合并，并将合并后的chunk放到unsorted bin中。注意，这里在合并的过程中，要更新chunk的大小，以反映合并后的chunk的大小。并转到第10步。

9) 如果执行到这一步，说明释放了一个与top chunk相邻的chunk。则无论它有多大，都将它与top chunk合并，并更新top chunk的大小等信息。转下一步。

10) 判断合并后的chunk 的大小是否大于FASTBIN_CONSOLIDATION_THRESHOLD（默认64KB），如果是的话，则会触发进行fast bins的合并操作，fast bins中的chunk将被遍历，并与相邻的空闲chunk进行合并，合并后的chunk会被放到unsorted bin中。fast bins将变为空，操作完成之后转下一步。

11) 判断top chunk的大小是否大于mmap收缩阈值（默认为128KB），如果是的话，对于主分配区，则会试图归还top chunk中的一部分给操作系统。但是最先分配的128KB空间是不会归还的，ptmalloc 会一直管理这部分内存，用于响应用户的分配请求；如果为非主分配区，会进行sub-heap收缩，将top chunk的一部分返回给操作系统，如果top chunk为整个sub-heap，会把整个sub-heap还回给操作系统。做完这一步之后，释放结束，从 free() 函数退出。可以看出，收缩堆的条件是当前free的chunk大小加上前后能合并chunk的大小大于64k，并且要top chunk的大小要达到mmap收缩阈值，才有可能收缩堆。

glibc使用中的一些问题
后分配的内存先释放，因为ptmalloc收缩内存是从top chunk开始，如果与top chunk相邻的chunk不能释放，top chunk以下的chunk都无法释放。

Ptmalloc不适合用于管理长生命周期的内存，特别是持续不定期分配和释放长生命周期的内存，这将导致ptmalloc内存暴增。如果要用ptmalloc分配长周期内存，在32位系统上，分配的内存块最好大于512K，64位系统上，分配的内存块大小大于32MB。这是由于ptmalloc默认开启mmap分配阈值动态调整功能，512K是32位系统mmap分配阈值的最大值，32MB是64位系统mmap分配阈值的最大值，这样可以保证ptmalloc分配的内存一定是从mmap映射区域分配的，当free时，ptmalloc会直接把该内存返回给操作系统，避免了被ptmalloc缓存。

不要关闭ptmalloc的mmap分配阈值动态调整机制，因为这种机制保证了短生命周期的内存分配尽量从ptmalloc缓存的内存chunk中分配，更高效，浪费更少的内存。如果关闭了该机制，对大于128KB的内存分配就会使用系统调用mmap向操作系统分配内存，使用系统调用分配内存一般会比从ptmalloc缓存的chunk中分配内存慢，特别是在多线程同时分配大内存块时，操作系统会串行调用mmap()，并为发生缺页异常的页加载新物理页时，默认强制清0。频繁使用mmap向操作系统分配内存是相当低效的。使用mmap分配的内存只适合长生命周期的大内存块。

多线程分阶段执行的程序不适合用ptmalloc，这种程序的内存更适合用内存池管理，就像Appach那样，每个连接请求处理分为多个阶段，每个阶段都有自己的内存池，每个阶段完成后，将相关的内存就返回给相关的内存池。Google的许多应用也是分阶段执行的，他们在使用ptmalloc也遇到了内存暴增的相关问题，于是他们实现了TCMalloc来代替ptmalloc，TCMalloc具有内存池的优点，又有垃圾回收的机制，并最大限度优化了锁的争用，并且空间利用率也高于ptmalloc。Ptmalloc假设了线程A释放的内存块能在线程B中得到重用，但B不一定会分配和A线程同样大小的内存块，于是就需要不断地做切割和合并，可能导致内存碎片。

尽量减少程序的线程数量和避免频繁分配/释放内存，Ptmalloc在多线程竞争激烈的情况下，首先查看线程私有变量是否存在分配区，如果存在则尝试加锁，如果加锁不成功会尝试其它分配区，如果所有的分配区的锁都被占用着，就会增加一个非主分配区供当前线程使用。由于在多个线程的私有变量中可能会保存同一个分配区，所以当线程较多时，加锁的代价就会上升，ptmalloc分配和回收内存都要对分配区加锁，从而导致了多线程竞争环境下ptmalloc的效率降低。

防止内存泄露，ptmalloc对内存泄露是相当敏感的，根据它的内存收缩机制，如果与top chunk相邻的那个chunk没有回收，将导致top chunk一下很多的空闲内存都无法返回给操作系统。

防止程序分配过多内存，或是由于Glibc内存暴增，导致系统内存耗尽，程序因OOM(Out Of Memory)被系统杀掉。预估程序可以使用的最大物理内存大小，配置系统的/proc/sys/vm/overcommit_memory，/proc/sys/vm/overcommit_ratio，以及使用ulimt –v限制程序能使用虚拟内存空间大小，防止程序因OOM被杀掉。

内存定位相关API
struct mallinfo
函数签名如下：

#include <malloc.h>
struct mallinfo mallinfo(void);
The mallinfo() function returns a copy of a structure containing information about memory allocations performed by malloc and related functions.  This structure is defined as follows:

struct mallinfo {
               int arena;     /* Non-mmapped space allocated (bytes) */  -- heap
               int ordblks;   /* Number of free chunks */
               int smblks;    /* Number of free fastbin blocks */
               int hblks;     /* Number of mmapped regions */
               int hblkhd;    /* Space allocated in mmapped regions (bytes) */  -- map
               int usmblks;   /* Maximum total allocated space (bytes) */
               int fsmblks;   /* Space in freed fastbin blocks (bytes) */
               int uordblks;  /* Total allocated space (bytes) */   -- in use
               int fordblks;  /* Total free space (bytes) */  -- 空洞
               int keepcost;  /* Top-most, releasable space (bytes) */
           };
从这个函数返回的结构体来分析：
Arena主要表示Heap上面分配的内存，hblks表示在mmap映射区上面分配的内存，uordblks是程序实际使用的内存，而fordblks就应该对应着内存空洞。
该函数返回返回heap(main_arena)的内存使用情况，不统计非主分配区的内存情况。

malloc_stats函数
#include <malloc.h>
void malloc_stats(void);
The malloc_stats() function prints (on standard error) statistics about memory allocated by malloc and related functions. For each arena (allocation area), this function prints the total amount of memory allocated and the total number of bytes consumed by in-use allocations. (These two values correspond to the arena and uordblks fields retrieved by mallinfo.) In addition, the function prints the sum of these two statistics for all arenas, and the maximum number of blocks and bytes that were ever simultaneously allocated using mmap.

该函数将glibc中所有分配区的内存使用情况都输出到stderr，可以通过freopen重定向到指定的文件中去。

输出的样例如下：

Arena 0:
system bytes     =   29634560
in use bytes     =   29211840

Arena 1:
system bytes     =   61935616
in use bytes     =   61141240

Arena 2:
system bytes     =   12419072
in use bytes     =   11500368

Total (incl. mmap):
system bytes     =  683708416
in use bytes     =  681572616
max mmap regions =         34
max mmap bytes   =  594857984

其中Arena表示分配区
system bytes表示glibc分配的总虚拟内存，in use bytes表示正在被使用的虚拟内存。
malloc_info 函数
#include <malloc.h>
int malloc_info(int options, FILE *fp);
The malloc_info() function exports an XML string that describes the current state of the memory-allocation implementation in the caller.
The string is printed on the file stream fp. The exported string includes information about all arenas
As currently implemented, options must be zero。

<malloc version="1">
       <heap nr="0">
              <sizes>
              </sizes>
              <total type="fast" count="0" size="0"/>
              <total type="rest" count="0" size="0"/>
              <system type="current" size="1081344"/>
              <system type="max" size="1081344"/>
              <aspace type="total" size="1081344"/>
              <aspace type="mprotect" size="1081344"/>
       </heap>
       <heap nr="1">
              <sizes>
              </sizes>
              <total type="fast" count="0" size="0"/>
              <total type="rest" count="0" size="0"/>
              <system type="current" size="1032192"/>
              <system type="max" size="1032192"/>
              <aspace type="total" size="1032192"/>
              <aspace type="mprotect" size="1032192"/>
       </heap>
       <total type="fast" count="0" size="0"/>
       <total type="rest" count="0" size="0"/>
       <system type="current" size="2113536"/>
       <system type="max" size="2113536"/>
       <aspace type="total" size="2113536"/>
       <aspace type="mprotect" size="2113536"/>
</malloc>
Glibc的Hook函数
#include <malloc.h>
void *(*__malloc_hook)(size_t size, const void *caller);
void *(*__realloc_hook)(void *ptr, size_t size", const void *" caller );

void *(*__memalign_hook)(size_t alignment, size_t size,
         const void *caller);
void (*__free_hook)(void *ptr, const void *caller);
void (*__malloc_initialize_hook)(void);
void (*__after_morecore_hook)(void);
具体用法可以参见：http://linux.die.net/man/3/__malloc_hook

linux相关内存定位命令
vmstat命令
vmstat命令是最常见的Linux/Unix监控工具，可以展现给定时间间隔的服务器的状态值,包括服务器的CPU使用率，内存使用，虚拟内存交换情况,IO读写情况。

一般vmstat工具的使用是通过两个数字参数来完成的，第一个参数是采样的时间间隔数，单位是秒，第二个参数是采样的次数，如:

root@ubuntu:~# vmstat 2 1
procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa
 1  0   0 3498472 315836 3819540    0    0     0     1    2    0  0  0 100  0
image.png

pmap命令
名称：
pmap - report memory map of a process(查看进程的内存映像信息)

扩展格式和设备格式域：

        Address:  start address of map  映像起始地址
        Kbytes:  size of map in kilobytes  映像大小
        RSS:  resident set size in kilobytes  驻留集大小
        Dirty:  dirty pages (both shared and private) in kilobytes  脏页大小
        Mode:  permissions on map 映像权限: r=read, w=write, x=execute, s=shared, p=private (copy on write)  
        Mapping:  file backing the map , or '[ anon ]' for allocated memory, or '[ stack ]' for the program stack.  映像支持文件,[anon]为已分配内存 [stack]为程序堆栈
        Offset:  offset into the file  文件偏移
        Device:  device name (major:minor)  设备名
[root@C44 ~]#  pmap -d 1
1:   init [5]                    
Address   Kbytes Mode  Offset           Device    Mapping
00934000      88 r-x-- 0000000000000000 008:00005 ld-2.3.4.so
0094a000       4 r---- 0000000000015000 008:00005 ld-2.3.4.so
0094b000       4 rw--- 0000000000016000 008:00005 ld-2.3.4.so
0094e000    1188 r-x-- 0000000000000000 008:00005 libc-2.3.4.so
00a77000       8 r---- 0000000000129000 008:00005 libc-2.3.4.so
00a79000       8 rw--- 000000000012b000 008:00005 libc-2.3.4.so
00a7b000       8 rw--- 0000000000a7b000 000:00000   [ anon ]
00a85000      52 r-x-- 0000000000000000 008:00005 libsepol.so.1
00a92000       4 rw--- 000000000000c000 008:00005 libsepol.so.1
00a93000      32 rw--- 0000000000a93000 000:00000   [ anon ]
00d9d000      52 r-x-- 0000000000000000 008:00005 libselinux.so.1
00daa000       4 rw--- 000000000000d000 008:00005 libselinux.so.1
08048000      28 r-x-- 0000000000000000 008:00005 init
0804f000       4 rw--- 0000000000007000 008:00005 init
084e1000     132 rw--- 00000000084e1000 000:00000   [ anon ]
b7f5d000       8 rw--- 00000000b7f5d000 000:00000   [ anon ]
bffee000      72 rw--- 00000000bffee000 000:00000   [ stack ]
ffffe000       4 ----- 0000000000000000 000:00000   [ anon ]
mapped: 1700K    writeable/private: 276K    shared: 0K
在输出样例中：
mapped 表示该进程映射的虚拟地址空间大小，也就是该进程预先分配的虚拟内存大小，即ps出的vsz
writeable/private 表示进程所占用的私有地址空间大小，也就是该进程实际使用的内存大小 shared 表示进程和其他进程共享的内存大小
